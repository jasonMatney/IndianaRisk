{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns  #Seaborn is another powerful visulization library for Python\n",
    "from sklearn.linear_model import Ridge\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\jmatney\\Documents\\GitHub\\IndianaRisk\\data\"\n",
    "df = pd.read_csv(os.path.join(path, \"IndianaRisk.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df.loc[:, df.columns != 'subwatershed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a variable for each type of feature\n",
    "target = [\"claims_total_building_insurance_coverage_avg\"]\n",
    "columns = [x for x in df if x != \"claims_total_building_insurance_coverage_avg\"]\n",
    "numeric_columns = [x for x in columns if x != \"subwatershed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Fixed dimensions\n",
    "input_dim = df_data.shape[1]  # 8\n",
    "encoding_dim = 3\n",
    "# Number of neurons in each Layer [8, 6, 4, 3, ...] of encoders\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder_layer_1 = Dense(6, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder_layer_2 = Dense(4, activation=\"tanh\")(encoder_layer_1)\n",
    "encoder_layer_3 = Dense(encoding_dim, activation=\"tanh\")(encoder_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear encoder model\n",
    "encoder = Model(inputs=input_layer, outputs=encoder_layer_3)\n",
    "# Use the model to predict the factors which sum up the information of interest rates.\n",
    "encoded_data = pd.DataFrame(encoder.predict(data_scaled))\n",
    "encoded_data.columns = ['factor_1', 'factor_2', 'factor_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor_1</th>\n",
       "      <th>factor_2</th>\n",
       "      <th>factor_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125464</td>\n",
       "      <td>-0.218238</td>\n",
       "      <td>-0.056406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.344653</td>\n",
       "      <td>-0.174668</td>\n",
       "      <td>-0.030308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112020</td>\n",
       "      <td>-0.242349</td>\n",
       "      <td>-0.158765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.129562</td>\n",
       "      <td>-0.140144</td>\n",
       "      <td>-0.051501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020781</td>\n",
       "      <td>0.020766</td>\n",
       "      <td>0.214785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.523242</td>\n",
       "      <td>-0.192554</td>\n",
       "      <td>-0.172746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.135218</td>\n",
       "      <td>-0.097426</td>\n",
       "      <td>-0.103977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.320945</td>\n",
       "      <td>0.054176</td>\n",
       "      <td>-0.123980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.407396</td>\n",
       "      <td>-0.264483</td>\n",
       "      <td>-0.263009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.323399</td>\n",
       "      <td>-0.166396</td>\n",
       "      <td>-0.174503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    factor_1  factor_2  factor_3\n",
       "0   0.125464 -0.218238 -0.056406\n",
       "1   0.344653 -0.174668 -0.030308\n",
       "2   0.112020 -0.242349 -0.158765\n",
       "3   0.129562 -0.140144 -0.051501\n",
       "4   0.020781  0.020766  0.214785\n",
       "..       ...       ...       ...\n",
       "95  0.523242 -0.192554 -0.172746\n",
       "96  0.135218 -0.097426 -0.103977\n",
       "97  0.320945  0.054176 -0.123980\n",
       "98  0.407396 -0.264483 -0.263009\n",
       "99  0.323399 -0.166396 -0.174503\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_data.loc[:, df_data.columns != 'claims_total_building_insurance_coverage_avg']\n",
    "y = df_data.loc[:, df_data.columns == 'claims_total_building_insurance_coverage_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# create target scaler object\n",
    "scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
    "scalarX.fit(X)\n",
    "scalarY.fit(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (72, 40)\n",
      "Shape of x_val: (8, 40)\n",
      "Shape of x_test: (20, 40)\n",
      "Shape of y_train: (72, 1)\n",
      "Shape of y_val: (8, 1)\n",
      "Shape of y_test: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "#Create train and test dataset with an 80:20 split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_data[numeric_columns],df_data[target],test_size=0.2,random_state=2018)\n",
    "# Further divide training dataset into train and validation dataset with an 90:10 split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1,random_state=2018)\n",
    "\n",
    "# transform target variables\n",
    "y_train = scalarY.transform(y_train)\n",
    "y_test = scalarY.transform(y_test)\n",
    "y_val = scalarY.transform(y_val)\n",
    "\n",
    "x_train = scalarX.transform(x_train)\n",
    "x_test = scalarX.transform(x_test)\n",
    "x_val = scalarX.transform(x_val)\n",
    "\n",
    "#Check the sizes of all newly created datasets\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-801f75f04fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=4)]\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(350,input_dim = 40,activation=\"relu\"))\n",
    "model4.add(Dropout(0.2, input_shape=(40,)))\n",
    "model4.add(Dense(350,activation=\"relu\"))\n",
    "model4.add(Dropout(0.2, input_shape=(40,)))\n",
    "model4.add(Dense(350,activation=\"relu\"))\n",
    "model4.add(Dropout(0.2, input_shape=(40,)))\n",
    "model4.add(Dense(350,activation=\"relu\"))\n",
    "model4.add(Dropout(0.2, input_shape=(40,)))\n",
    "model4.add(Dense(350,activation=\"relu\"))\n",
    "model4.add(Dropout(0.2, input_shape=(40,)))\n",
    "model4.add(Dense(1,activation = \"linear\"))\n",
    "\n",
    "model4.compile(optimizer='adam',loss=\"mean_squared_error\",\n",
    "               metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "model4.fit(x_train,y_train, \n",
    "           validation_data=(x_val,y_val), \n",
    "           epochs=450,\n",
    "           batch_size=64)\n",
    "#,callbacks=callbacks)\n",
    "\n",
    "result = model4.evaluate(x_test,y_test)\n",
    "\n",
    "for i in range(len(model4.metrics_names)):\n",
    "    print(\"Metric \",model4.metrics_names[i],\":\",str(round(result[i],2)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
